{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "This Notebook reads pre-processed predictor (SWE) and predictand (discharge volumes) data. It then further processes the predictor into principal components using a Principal Component Analysis (PCA). It then uses the principal components as inputs to an Ordinary Least Squares (OLS) regression model to produce ensemble hindcasts (retrospective forecasts) of the predictand. Note that in this workflow we might use the terms forecast and hindcast interchangeably as they would be generated the same way with this method, but either for the future or in hindsight, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decisions:\n",
    "- We use SWE data on the 1st of each month only for forecasting.\n",
    "- As a result of the PCA design, we assume that we will use all PCs monthly data independently from other months for the forecasting. This is to ensure that we maximize the amount of data we can use each month. E.g., 1st April has more data than 1st November and we would have to drop all the additional data in April if we were to unify the PCA across months, as PCA does not allow for any missing data.\n",
    "- We use the first SWE principal component only as a predictor for forecasting (see user-specified variables below). If using more PCs, we should be careful with overfitting when the dataset has a few years compared to the number of PCs.\n",
    "- We use a leave-one-out strategy for cross-validation of the model (see user-specified variables below).\n",
    "- We use an OLS regression model. This could be replaced with other models in the future.\n",
    "- We generate ensemble hindcasts with 100 ensemble members (see user-specified variables below).\n",
    "- The ensemble members are generated with an even distribution (vs. random; see user-specified variables below).\n",
    "\n",
    "The \"Variables\" section below is the only section a user will need to modify for testing different options for most of these decisions.\n",
    "\n",
    "Notes:\n",
    "- We do not look at input data stationarity.\n",
    "- We are keeping all data available to build the models, but we could decide to discard extreme years for training the forecast model, as including them could skew the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules, settings & functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:27:05.117680Z",
     "start_time": "2024-04-10T02:27:05.110149Z"
    }
   },
   "source": [
    "# Import required modules\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "import logging\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sys\n",
    "import xarray as xr"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:27:05.353359Z",
     "start_time": "2024-04-10T02:27:05.235643Z"
    }
   },
   "source": [
    "# Add scripts to the system path\n",
    "sys.path.append('../scripts')\n",
    "\n",
    "# Set up logging, configured for this workflow (see utilities.py)\n",
    "from utilities import setup_logging, read_settings\n",
    "setup_logging()\n",
    "\n",
    "# Set up logging for this notebook\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Suppress misc. comments from being added to the log file\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "logging.getLogger('matplotlib.pyplot').disabled = True\n",
    "\n",
    "# Get the logger for fiona._env and suppress everything below CRITICAL level\n",
    "fiona_env_logger = logging.getLogger('fiona._env')\n",
    "fiona_env_logger.setLevel(logging.CRITICAL)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 12:27:05,275 - root - INFO - Logging setup complete. Log file: /Users/hasithaj/PycharmProjects/FROSTBYTE/logs/data_driven_forecasting_20240410_122705.log\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:27:05.384754Z",
     "start_time": "2024-04-10T02:27:05.357173Z"
    }
   },
   "source": [
    "# Save Notebook name to the log file\n",
    "logger.debug(f'Notebook: 4_Forecasting')"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:27:05.411602Z",
     "start_time": "2024-04-10T02:27:05.386393Z"
    }
   },
   "source": [
    "# Read settings file\n",
    "settings = read_settings('../settings/config_test_case.yaml', log_settings=True)\n",
    "pprint(settings)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-10 12:27:05,409 - root - INFO - Settings logged from ../settings/config_test_case.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SWE_obs_path': '../test_case_data/input_data/SWE_1979_2022_testcases.nc',\n",
      " 'basins_dem_path': '../test_case_data/input_data/MERIT_Hydro_dem_',\n",
      " 'basins_shp_path': '../test_case_data/input_data/basins_testcases.shp',\n",
      " 'domain': '05BB001',\n",
      " 'output_data_path': '../test_case_data/output_data/',\n",
      " 'plots_path': '../test_case_data/output_plots/',\n",
      " 'precip_obs_path': '../test_case_data/input_data/SCDNA_v1.1_testcases.nc',\n",
      " 'streamflow_obs_path': '../test_case_data/input_data/Qobs_1979_2021_testcases.nc'}\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:27:08.729076Z",
     "start_time": "2024-04-10T02:27:05.415114Z"
    }
   },
   "source": [
    "# Import required functions\n",
    "from functions import deterministic_forecasting, ensemble_dressing, ensemble_forecasting, leave_out, OLS_model_fitting, principal_component_analysis"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:27:08.776034Z",
     "start_time": "2024-04-10T02:27:08.731338Z"
    }
   },
   "source": [
    "# Set user-specified variables\n",
    "test_basin_id = settings['domain'] # Can override this with testbasin_id = <string of the testbasin id>, make sure that this id is in the input data files\n",
    "PC_no_default, PC_id_default = 1, 'PC1'  # integer > 0 for the number of principal components of SWE to use for the forecasting, and string of the PC to use (if PC_no > 1, PC_id should be a list of strings)\n",
    "target_periods = ['01/01-30/09','01/02-30/09','01/03-30/09','01/04-30/09','01/05-30/09','01/06-30/09','01/07-30/09','01/08-30/09','01/09-30/09']  # target periods for predictand, where each period is described as 'start_DD/start_MM-end_DD/end_MM'\n",
    "init_dates = ['01/01','01/02','01/03','01/04','01/05','01/06','01/07','01/08','01/09'] # initialization dates for predictor, where each date is described as 'DD/MM'\n",
    "min_obs_corr_default = 3 # minimum number of observations required to calculate the correlation between predictand-predictor\n",
    "min_years_overlap_default = 10 # minimum number of years required of predictor-predictand to be able to generate a forecast\n",
    "nyears_leaveout_default = 1 # number of years to leave out at a time for forecast cross-validation\n",
    "method_traintest_default = 'leave_out' # method to use for the cross-validation - no other methods are implemented at this stage\n",
    "ens_size_default = 100  # number of forecast ensemble members to generate\n",
    "test_target_period = '01/06-30/09'  # target period used for the workflow step-by-step demonstration\n",
    "test_init_date = '01/05' # initialization date used for the workflow step-by-step demonstration"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:27:08.801429Z",
     "start_time": "2024-04-10T02:27:08.776650Z"
    }
   },
   "source": [
    "# Save the user-specified variables to the log file\n",
    "logger.debug(f'test basin ID: {test_basin_id}')\n",
    "logger.debug(f'PCs used as predictors for forecasting: {PC_id_default}')\n",
    "logger.debug(f'forecast target periods: {target_periods}')\n",
    "logger.debug(f'forecast initialization dates: {init_dates}')\n",
    "logger.debug(f'min. number of obs. for correlation calculation: {min_obs_corr_default}')\n",
    "logger.debug(f'min. number of predictor-predictand for forecast generation: {min_years_overlap_default}')\n",
    "logger.debug(f'number of years left out at a time for cross-validation: {nyears_leaveout_default}')\n",
    "logger.debug(f'forecast ensemble size: {ens_size_default}')"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:27:09.100724Z",
     "start_time": "2024-04-10T02:27:08.803317Z"
    }
   },
   "source": [
    "# Read pre-processed predictand data & format to Pandas DataFrame for forecasting\n",
    "predictand_ds = xr.open_dataset(settings['output_data_path']+\"Vol_1979_2021_basin\"+test_basin_id+\".nc\")\n",
    "predictand_da = predictand_ds.sel(Station_ID=test_basin_id)\n",
    "predictand_df = predictand_da.to_dataframe().reset_index().drop(columns=['lat','lon','source','Station_ID']).set_index('year')\n",
    "\n",
    "display(predictand_df.head())"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      Vol_1Jan-30Sep  Vol_1Feb-30Sep  Vol_1Mar-30Sep  Vol_1Apr-30Sep  \\\n",
       "year                                                                   \n",
       "1979    8.980321e+08    8.712965e+08    8.544467e+08    8.357645e+08   \n",
       "1980    1.083993e+09    1.063606e+09    1.044934e+09    1.028865e+09   \n",
       "1981    1.290590e+09    1.261888e+09    1.240591e+09    1.217553e+09   \n",
       "1982    1.100052e+09    1.076052e+09    1.050373e+09    1.026711e+09   \n",
       "1983    9.657161e+08    9.375100e+08    9.156214e+08    8.961892e+08   \n",
       "\n",
       "      Vol_1May-30Sep  Vol_1Jun-30Sep  Vol_1Jul-30Sep  Vol_1Aug-30Sep  \\\n",
       "year                                                                   \n",
       "1979    8.166528e+08    7.280237e+08    4.564771e+08    2.212790e+08   \n",
       "1980    9.906106e+08    7.140269e+08    4.022611e+08    2.096496e+08   \n",
       "1981    1.190480e+09    9.881568e+08    7.342272e+08    3.495571e+08   \n",
       "1982    1.005872e+09    9.266832e+08    5.734195e+08    2.941402e+08   \n",
       "1983    8.710589e+08    7.629379e+08    5.065286e+08    2.531520e+08   \n",
       "\n",
       "      Vol_1Sep-30Sep  \n",
       "year                  \n",
       "1979    8.181216e+07  \n",
       "1980    8.949312e+07  \n",
       "1981    1.083542e+08  \n",
       "1982    1.251850e+08  \n",
       "1983    8.290944e+07  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vol_1Jan-30Sep</th>\n",
       "      <th>Vol_1Feb-30Sep</th>\n",
       "      <th>Vol_1Mar-30Sep</th>\n",
       "      <th>Vol_1Apr-30Sep</th>\n",
       "      <th>Vol_1May-30Sep</th>\n",
       "      <th>Vol_1Jun-30Sep</th>\n",
       "      <th>Vol_1Jul-30Sep</th>\n",
       "      <th>Vol_1Aug-30Sep</th>\n",
       "      <th>Vol_1Sep-30Sep</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>8.980321e+08</td>\n",
       "      <td>8.712965e+08</td>\n",
       "      <td>8.544467e+08</td>\n",
       "      <td>8.357645e+08</td>\n",
       "      <td>8.166528e+08</td>\n",
       "      <td>7.280237e+08</td>\n",
       "      <td>4.564771e+08</td>\n",
       "      <td>2.212790e+08</td>\n",
       "      <td>8.181216e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>1.083993e+09</td>\n",
       "      <td>1.063606e+09</td>\n",
       "      <td>1.044934e+09</td>\n",
       "      <td>1.028865e+09</td>\n",
       "      <td>9.906106e+08</td>\n",
       "      <td>7.140269e+08</td>\n",
       "      <td>4.022611e+08</td>\n",
       "      <td>2.096496e+08</td>\n",
       "      <td>8.949312e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>1.290590e+09</td>\n",
       "      <td>1.261888e+09</td>\n",
       "      <td>1.240591e+09</td>\n",
       "      <td>1.217553e+09</td>\n",
       "      <td>1.190480e+09</td>\n",
       "      <td>9.881568e+08</td>\n",
       "      <td>7.342272e+08</td>\n",
       "      <td>3.495571e+08</td>\n",
       "      <td>1.083542e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>1.100052e+09</td>\n",
       "      <td>1.076052e+09</td>\n",
       "      <td>1.050373e+09</td>\n",
       "      <td>1.026711e+09</td>\n",
       "      <td>1.005872e+09</td>\n",
       "      <td>9.266832e+08</td>\n",
       "      <td>5.734195e+08</td>\n",
       "      <td>2.941402e+08</td>\n",
       "      <td>1.251850e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>9.657161e+08</td>\n",
       "      <td>9.375100e+08</td>\n",
       "      <td>9.156214e+08</td>\n",
       "      <td>8.961892e+08</td>\n",
       "      <td>8.710589e+08</td>\n",
       "      <td>7.629379e+08</td>\n",
       "      <td>5.065286e+08</td>\n",
       "      <td>2.531520e+08</td>\n",
       "      <td>8.290944e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We're only showing the first few rows of data, otherwise it takes too much space. Same for the predictors below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-10T02:27:09.518224Z",
     "start_time": "2024-04-10T02:27:09.101366Z"
    }
   },
   "source": [
    "# Read pre-processed predictor data for basin of interest & format to Pandas DataFrame for forecasting\n",
    "predictor_ds = xr.open_dataset(settings['output_data_path']+\"SWE_1979_2022_gapfilled_basin\"+test_basin_id+\".nc\")\n",
    "predictor_df = predictor_ds.to_dataframe().drop(columns=['flag','donor_stations','lat','lon','station_name']).unstack(level='station_id')\n",
    "predictor_df.columns = predictor_df.columns.droplevel()\n",
    "\n",
    "display(predictor_df.head())"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hasithaj/PycharmProjects/FROSTBYTE/test_case_data/output_data/SWE_1979_2022_gapfilled_basin05BB001.nc'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:211\u001B[0m, in \u001B[0;36mCachingFileManager._acquire_with_cache_info\u001B[0;34m(self, needs_lock)\u001B[0m\n\u001B[1;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 211\u001B[0m     file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cache[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_key]\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/lru_cache.py:56\u001B[0m, in \u001B[0;36mLRUCache.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[0;32m---> 56\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cache[key]\n\u001B[1;32m     57\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_cache\u001B[38;5;241m.\u001B[39mmove_to_end(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('/Users/hasithaj/PycharmProjects/FROSTBYTE/test_case_data/output_data/SWE_1979_2022_gapfilled_basin05BB001.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), 'f0bfde20-9b15-4294-8371-da8d4fcb0fb6']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Read pre-processed predictor data for basin of interest & format to Pandas DataFrame for forecasting\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m predictor_ds \u001B[38;5;241m=\u001B[39m xr\u001B[38;5;241m.\u001B[39mopen_dataset(settings[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124moutput_data_path\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSWE_1979_2022_gapfilled_basin\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39mtest_basin_id\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.nc\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m predictor_df \u001B[38;5;241m=\u001B[39m predictor_ds\u001B[38;5;241m.\u001B[39mto_dataframe()\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mflag\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdonor_stations\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlat\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlon\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstation_name\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39munstack(level\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstation_id\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m predictor_df\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;241m=\u001B[39m predictor_df\u001B[38;5;241m.\u001B[39mcolumns\u001B[38;5;241m.\u001B[39mdroplevel()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/api.py:573\u001B[0m, in \u001B[0;36mopen_dataset\u001B[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001B[0m\n\u001B[1;32m    561\u001B[0m decoders \u001B[38;5;241m=\u001B[39m _resolve_decoders_kwargs(\n\u001B[1;32m    562\u001B[0m     decode_cf,\n\u001B[1;32m    563\u001B[0m     open_backend_dataset_parameters\u001B[38;5;241m=\u001B[39mbackend\u001B[38;5;241m.\u001B[39mopen_dataset_parameters,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    569\u001B[0m     decode_coords\u001B[38;5;241m=\u001B[39mdecode_coords,\n\u001B[1;32m    570\u001B[0m )\n\u001B[1;32m    572\u001B[0m overwrite_encoded_chunks \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite_encoded_chunks\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 573\u001B[0m backend_ds \u001B[38;5;241m=\u001B[39m backend\u001B[38;5;241m.\u001B[39mopen_dataset(\n\u001B[1;32m    574\u001B[0m     filename_or_obj,\n\u001B[1;32m    575\u001B[0m     drop_variables\u001B[38;5;241m=\u001B[39mdrop_variables,\n\u001B[1;32m    576\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdecoders,\n\u001B[1;32m    577\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    578\u001B[0m )\n\u001B[1;32m    579\u001B[0m ds \u001B[38;5;241m=\u001B[39m _dataset_from_backend_dataset(\n\u001B[1;32m    580\u001B[0m     backend_ds,\n\u001B[1;32m    581\u001B[0m     filename_or_obj,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    591\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    592\u001B[0m )\n\u001B[1;32m    593\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ds\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:646\u001B[0m, in \u001B[0;36mNetCDF4BackendEntrypoint.open_dataset\u001B[0;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001B[0m\n\u001B[1;32m    625\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mopen_dataset\u001B[39m(  \u001B[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001B[39;00m\n\u001B[1;32m    626\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    627\u001B[0m     filename_or_obj: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m|\u001B[39m os\u001B[38;5;241m.\u001B[39mPathLike[Any] \u001B[38;5;241m|\u001B[39m BufferedIOBase \u001B[38;5;241m|\u001B[39m AbstractDataStore,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    643\u001B[0m     autoclose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[1;32m    644\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dataset:\n\u001B[1;32m    645\u001B[0m     filename_or_obj \u001B[38;5;241m=\u001B[39m _normalize_path(filename_or_obj)\n\u001B[0;32m--> 646\u001B[0m     store \u001B[38;5;241m=\u001B[39m NetCDF4DataStore\u001B[38;5;241m.\u001B[39mopen(\n\u001B[1;32m    647\u001B[0m         filename_or_obj,\n\u001B[1;32m    648\u001B[0m         mode\u001B[38;5;241m=\u001B[39mmode,\n\u001B[1;32m    649\u001B[0m         \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m,\n\u001B[1;32m    650\u001B[0m         group\u001B[38;5;241m=\u001B[39mgroup,\n\u001B[1;32m    651\u001B[0m         clobber\u001B[38;5;241m=\u001B[39mclobber,\n\u001B[1;32m    652\u001B[0m         diskless\u001B[38;5;241m=\u001B[39mdiskless,\n\u001B[1;32m    653\u001B[0m         persist\u001B[38;5;241m=\u001B[39mpersist,\n\u001B[1;32m    654\u001B[0m         lock\u001B[38;5;241m=\u001B[39mlock,\n\u001B[1;32m    655\u001B[0m         autoclose\u001B[38;5;241m=\u001B[39mautoclose,\n\u001B[1;32m    656\u001B[0m     )\n\u001B[1;32m    658\u001B[0m     store_entrypoint \u001B[38;5;241m=\u001B[39m StoreBackendEntrypoint()\n\u001B[1;32m    659\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m close_on_error(store):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:409\u001B[0m, in \u001B[0;36mNetCDF4DataStore.open\u001B[0;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001B[0m\n\u001B[1;32m    403\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\n\u001B[1;32m    404\u001B[0m     clobber\u001B[38;5;241m=\u001B[39mclobber, diskless\u001B[38;5;241m=\u001B[39mdiskless, persist\u001B[38;5;241m=\u001B[39mpersist, \u001B[38;5;28mformat\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mformat\u001B[39m\n\u001B[1;32m    405\u001B[0m )\n\u001B[1;32m    406\u001B[0m manager \u001B[38;5;241m=\u001B[39m CachingFileManager(\n\u001B[1;32m    407\u001B[0m     netCDF4\u001B[38;5;241m.\u001B[39mDataset, filename, mode\u001B[38;5;241m=\u001B[39mmode, kwargs\u001B[38;5;241m=\u001B[39mkwargs\n\u001B[1;32m    408\u001B[0m )\n\u001B[0;32m--> 409\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(manager, group\u001B[38;5;241m=\u001B[39mgroup, mode\u001B[38;5;241m=\u001B[39mmode, lock\u001B[38;5;241m=\u001B[39mlock, autoclose\u001B[38;5;241m=\u001B[39mautoclose)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:356\u001B[0m, in \u001B[0;36mNetCDF4DataStore.__init__\u001B[0;34m(self, manager, group, mode, lock, autoclose)\u001B[0m\n\u001B[1;32m    354\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_group \u001B[38;5;241m=\u001B[39m group\n\u001B[1;32m    355\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mode \u001B[38;5;241m=\u001B[39m mode\n\u001B[0;32m--> 356\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mformat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mds\u001B[38;5;241m.\u001B[39mdata_model\n\u001B[1;32m    357\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_filename \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mds\u001B[38;5;241m.\u001B[39mfilepath()\n\u001B[1;32m    358\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_remote \u001B[38;5;241m=\u001B[39m is_remote_uri(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_filename)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:418\u001B[0m, in \u001B[0;36mNetCDF4DataStore.ds\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[1;32m    417\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mds\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 418\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_acquire()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/netCDF4_.py:412\u001B[0m, in \u001B[0;36mNetCDF4DataStore._acquire\u001B[0;34m(self, needs_lock)\u001B[0m\n\u001B[1;32m    411\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_acquire\u001B[39m(\u001B[38;5;28mself\u001B[39m, needs_lock\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m--> 412\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_manager\u001B[38;5;241m.\u001B[39macquire_context(needs_lock) \u001B[38;5;28;01mas\u001B[39;00m root:\n\u001B[1;32m    413\u001B[0m         ds \u001B[38;5;241m=\u001B[39m _nc4_require_group(root, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_group, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mode)\n\u001B[1;32m    414\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m ds\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/contextlib.py:137\u001B[0m, in \u001B[0;36m_GeneratorContextManager.__enter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwds, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfunc\n\u001B[1;32m    136\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgen)\n\u001B[1;32m    138\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mStopIteration\u001B[39;00m:\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerator didn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt yield\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:199\u001B[0m, in \u001B[0;36mCachingFileManager.acquire_context\u001B[0;34m(self, needs_lock)\u001B[0m\n\u001B[1;32m    196\u001B[0m \u001B[38;5;129m@contextlib\u001B[39m\u001B[38;5;241m.\u001B[39mcontextmanager\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21macquire_context\u001B[39m(\u001B[38;5;28mself\u001B[39m, needs_lock\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[1;32m    198\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001B[39;00m\n\u001B[0;32m--> 199\u001B[0m     file, cached \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_acquire_with_cache_info(needs_lock)\n\u001B[1;32m    200\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    201\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m file\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/xarray/backends/file_manager.py:217\u001B[0m, in \u001B[0;36mCachingFileManager._acquire_with_cache_info\u001B[0;34m(self, needs_lock)\u001B[0m\n\u001B[1;32m    215\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m    216\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmode\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mode\n\u001B[0;32m--> 217\u001B[0m file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_opener(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    218\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    219\u001B[0m     \u001B[38;5;66;03m# ensure file doesn't get overridden when opened again\u001B[39;00m\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_mode \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32msrc/netCDF4/_netCDF4.pyx:2469\u001B[0m, in \u001B[0;36mnetCDF4._netCDF4.Dataset.__init__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32msrc/netCDF4/_netCDF4.pyx:2028\u001B[0m, in \u001B[0;36mnetCDF4._netCDF4._ensure_nc_success\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/hasithaj/PycharmProjects/FROSTBYTE/test_case_data/output_data/SWE_1979_2022_gapfilled_basin05BB001.nc'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hindcast generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow step-by-step demonstration\n",
    "Let's go over the forecasting steps for a test forecast start date and target period to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Define initialization date for which to produce hindcasts\n",
    "init_day, init_month = int(test_init_date[0:2]), int(test_init_date[3:5])\n",
    "init_month_name = datetime.datetime.strptime(str(init_month), \"%m\").strftime(\"%b\")\n",
    "    \n",
    "# Define target period for which to produce hindcasts\n",
    "target_start_day, target_start_month = int(test_target_period[0:2]), int(test_target_period[3:5])\n",
    "target_end_day, target_end_month = int(test_target_period[6:8]), int(test_target_period[9:11])\n",
    "target_start_month_name = datetime.datetime.strptime(str(target_start_month), \"%m\").strftime(\"%b\")\n",
    "target_end_month_name = datetime.datetime.strptime(str(target_end_month), \"%m\").strftime(\"%b\")\n",
    "\n",
    "print(\"We will generate hindcasts initialized on\",init_day, init_month_name, \"for the target period\", target_start_day, target_start_month_name,\"-\",target_end_day,target_end_month_name,\".\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select predictor of interest\n",
    "predictor_subset_df = predictor_df[(predictor_df.index.month == init_month) & (predictor_df.index.day == init_day)]\n",
    "\n",
    "display(predictor_subset_df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Again here we're only showing the first few rows of data, otherwise it takes too much space. Same for the predictand below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Select predictand of interest\n",
    "predictand_subset_df = predictand_df['Vol_'+str(target_start_day)+target_start_month_name+'-'+str(target_end_day)+target_end_month_name]\n",
    "\n",
    "display(predictand_subset_df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Clean predictor and predictand datasets and find the number of overlapping years with data\n",
    "cleaned_predictor_data = predictor_subset_df.dropna(axis=1,thresh=min_years_overlap_default).dropna(axis=0,how='any')\n",
    "cleaned_predictand_data = predictand_subset_df.dropna()\n",
    "\n",
    "if (cleaned_predictor_data.empty == False) and (cleaned_predictand_data.empty == False):\n",
    "    cleaned_predictor_data_years = cleaned_predictor_data.index.year.values\n",
    "    cleaned_predictand_data_years = cleaned_predictand_data.index.values\n",
    "    overlapping_years = list(set(cleaned_predictor_data_years) & set(cleaned_predictand_data_years))\n",
    "    overlapping_years.sort()\n",
    "else:\n",
    "    overlapping_years = []\n",
    "    \n",
    "overlapping_predictor_data = cleaned_predictor_data[cleaned_predictor_data.index.year.isin(overlapping_years)]\n",
    "overlapping_predictand_data = predictand_subset_df.loc[overlapping_years]\n",
    "    \n",
    "print(\"There are\",str(len(overlapping_years)),\"overlapping years with data between the predictors and the predictand for this starting date-target period combination.\")\n",
    "display(overlapping_predictor_data)\n",
    "display(overlapping_predictand_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a minimum number of years of data to be able to produce reliable hindcasts. min_years_overlap_default defines the minimum number of years requires. If this condition is met, we can proceed with the forecasting steps below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run a Principal Component Analysis (PCA), a statistical method used to transform a set of intercorrelated variables into an equal number of uncorrelated variables. This step becomes particularly essential after gap filling, which might have introduced additional correlation across the SWE stations."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run PCA\n",
    "PCs, loadings, fig = principal_component_analysis(overlapping_predictor_data, flag=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the variance in the predictor data (gap-filled SWE stations observations) captured by each principal component. Where the captured variance decreases with each new PC. For the Bow at Banff, we can see that the first principal component captures more than 90% of the variance. We will therefore use PC1 as the sole predictor for the rest of the forecasting process."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot PC1 vs. each stations' SWE\n",
    "if len(overlapping_predictor_data.columns) < 5:\n",
    "    fig, ax = plt.subplots(1,len(overlapping_predictor_data.columns), figsize=[9,2])\n",
    "    col = -1\n",
    "    for s in range(len(overlapping_predictor_data.columns)):\n",
    "        col += 1\n",
    "        ax[col].scatter(overlapping_predictor_data.iloc[:,s], PCs['PC1'], color='b', alpha=.3) \n",
    "        ax[col].tick_params(axis='x', labelsize=8)\n",
    "        ax[col].tick_params(axis='y', labelsize=8)\n",
    "        ax[col].set_xlabel(overlapping_predictor_data.columns[s], fontweight='bold')\n",
    "    ax[0].set_ylabel('PC1', fontweight='bold')\n",
    "    plt.tight_layout();\n",
    "    \n",
    "elif len(overlapping_predictor_data.columns) > 4:\n",
    "    nrow = int(len(overlapping_predictor_data.columns)/4)\n",
    "    ncol = 4\n",
    "    if len(overlapping_predictor_data.columns)%4 != 0:\n",
    "        nrow += 1\n",
    "    fig, ax = plt.subplots(nrow,ncol, figsize=[9,2*nrow])\n",
    "    row = 0\n",
    "    col = -1\n",
    "    for s in range(len(overlapping_predictor_data.columns)):\n",
    "        col += 1\n",
    "        if col == ncol:\n",
    "            row += 1\n",
    "            col = 0\n",
    "        ax[row,col].scatter(overlapping_predictor_data.iloc[:,s], PCs['PC1'], color='b', alpha=.3) \n",
    "        ax[row,col].tick_params(axis='x', labelsize=8)\n",
    "        ax[row,col].tick_params(axis='y', labelsize=8)\n",
    "        ax[row,col].set_xlabel(overlapping_predictor_data.columns[s], fontweight='bold')\n",
    "    for r in range(nrow):\n",
    "        ax[r,0].set_ylabel('PC1', fontweight='bold')\n",
    "    empties = 4*nrow - len(overlapping_predictor_data.columns)\n",
    "    for c in range(ncol-empties, ncol):\n",
    "        fig.delaxes(ax[nrow-1,c]);\n",
    "    plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows how well PC1 correlates with each individual station observations. Note than the correlations can be negative due to the SWE observations being standardized prior to the PCA. This however should not impact the next forecasting steps. Let's have a look at the spatial patterns in these correlations now."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Make map of PCA loadings (correlation between stations & PCs data)\n",
    "# Note that this takes a few seconds to plot as it needs to load the DEM\n",
    "\n",
    "# Load DEM\n",
    "src = rasterio.open(settings['basins_dem_path']+test_basin_id+\".tif\")\n",
    "\n",
    "# Read test basin's shapefile\n",
    "basins_gdf = gpd.read_file(settings['basins_shp_path'])\n",
    "shp_testbasin_gdf = basins_gdf.loc[basins_gdf.Station_ID == test_basin_id]\n",
    "\n",
    "# Add basin contour & elevation shading to map\n",
    "shp_testbasin_gdf.plot(edgecolor='k', facecolor='none', lw=.5)\n",
    "rasterio.plot.show((src, 1), cmap='Greys', vmin=0, alpha=.7)\n",
    "\n",
    "# Extract geospatial information for stations to plot\n",
    "SWE_stations_geos = predictor_ds.sel(station_id=loadings.columns)\n",
    "\n",
    "# plot data\n",
    "sc = plt.scatter(SWE_stations_geos.lon.values, SWE_stations_geos.lat.values, c=loadings.loc['PC1'].values, cmap='rocket_r')\n",
    "\n",
    "# Remove frame ticks\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(sc, fraction=.03)\n",
    "cbar.set_label('R$^2$');"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some spatial patterns in the signal picked up by PC1 across the river basin. We now plot a timeseries of all PCs and of the predictand to see what the temporal patterns are."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot all PCs and the predictand\n",
    "fig, ax = plt.subplots(2, 1, figsize=(8,6))\n",
    "for pc in range(len(PCs.columns)):\n",
    "    PCs.iloc[:,pc].plot(ax=ax[0], marker='o', label=PCs.iloc[:,pc].name)\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[0].set_ylabel('Standardized SWE PCs')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Predictors')\n",
    "overlapping_predictand_data.plot(ax=ax[1], marker='o')\n",
    "ax[1].set_xlabel('')\n",
    "ax[1].set_ylabel('Volumes [m$^3$]')\n",
    "ax[1].set_title('Predictand')\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot helps us understand check visually whether the PCs and the predictand follow a similar temporal behavior visually. We can see for the Bow River at Banff how PC1 has a clear signal that fluctuates over time, while the other PCs seem to have smaller values that are more noisy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Combine the PCs and the predictand into a single DataFrame for forecasting\n",
    "combined_df = PCs.reset_index(drop=True)\n",
    "combined_df['year'] = overlapping_years\n",
    "combined_df = combined_df.set_index('year')\n",
    "combined_df['Vol'] = overlapping_predictand_data\n",
    "\n",
    "display(combined_df.head())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now go over the model building and ensemble dressing steps in cross-validation mode. We will only print out the outputs for the last year left out and predicted for illustrative purposes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Split the timeseries into training and validation timeseries for forecasting\n",
    "train_data_dict, test_data_dict = leave_out(combined_df, nyears_leaveout_default)\n",
    "\n",
    "# Loop over the samples\n",
    "for s in list(train_data_dict.keys()):\n",
    "\n",
    "    # Select train and test data\n",
    "    train_data = train_data_dict[s]\n",
    "    test_data = test_data_dict[s]\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    OLS_model = OLS_model_fitting(PC_id_default, train_data)\n",
    "\n",
    "    # Perform out-of-sample deterministic forecasting for the testing period\n",
    "    fc_det = deterministic_forecasting(OLS_model, test_data)\n",
    "\n",
    "    # Calculate errors standard deviation for the training period\n",
    "    fc_det_train = deterministic_forecasting(OLS_model, train_data)\n",
    "    rmse = mean_squared_error(train_data['Vol'].values, fc_det_train['Vol_fc_mean'].values, squared=False)\n",
    "\n",
    "    # generate ensembles\n",
    "    fc_ens = ensemble_dressing(fc_det, rmse, ens_size=ens_size_default)\n",
    "\n",
    "    # append all ensembles generated for each moving window\n",
    "    if s == 0:\n",
    "        fc_ens_df = fc_ens\n",
    "    else:\n",
    "        fc_ens_df = pd.concat([fc_ens_df,fc_ens])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Print a summary of the model\n",
    "print(OLS_model.summary())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a breakfown of what the regression results mean, see this [post](https://medium.com/swlh/interpreting-linear-regression-through-statsmodels-summary-4796d359035a)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot the predictor, predictand & the errors standard deviation for the training period\n",
    "sorted_data = fc_det_train.sort_values(by='Vol_fc_mean')\n",
    "plt.scatter(train_data[PC_id_default], train_data['Vol'], color='r', label='observations')\n",
    "plt.plot(train_data[PC_id_default].loc[sorted_data.index], sorted_data['Vol_fc_mean'], color='b', label='regression line')\n",
    "plt.fill_between(train_data[PC_id_default].loc[sorted_data.index], sorted_data['Vol_fc_mean']+rmse, sorted_data['Vol_fc_mean']-rmse, color='purple', alpha=.1, label='errors SD')\n",
    "plt.xlabel('Standardized SWE PC1')\n",
    "plt.ylabel('Volume [m$^3$]')\n",
    "plt.legend();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shaded area shows the Standard Deviation (SD) of the errors between the observations and the regression line. This is used to generate ensembles around the deterministic forecast for the year left out, by drawing random samples from a normal (Gaussian) distribution within this space."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Plot timeseries of ensemble hindcasts and observations\n",
    "fig = plt.figure(figsize=(9,4))\n",
    "ts_ax = plt.subplot()\n",
    "obs, = ts_ax.plot(np.arange(1, len(overlapping_predictand_data.index)+1), overlapping_predictand_data.values, color='red', label='observations', marker='o')\n",
    "bp = plt.boxplot(np.transpose(fc_ens_df.values), patch_artist=True, zorder=1, whis=[0, 100], showfliers=False)\n",
    "plt.setp(bp['boxes'], color='b', alpha=.5)\n",
    "plt.setp(bp['whiskers'], color='b')\n",
    "plt.setp(bp['medians'], color='k')\n",
    "bluepatch = mpatches.Patch(color='b', alpha=.5, label='ensemble hindcasts')\n",
    "ts_ax.set_ylabel('Volume [m$^3$]')\n",
    "ts_ax.set_xticks(np.arange(1, len(overlapping_predictand_data.index)+1))\n",
    "ts_ax.set_xticklabels(overlapping_predictand_data.index.values, rotation=35, fontsize=8)\n",
    "plt.legend(handles=[obs,bluepatch])\n",
    "plt.tight_layout();"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate all hindcasts\n",
    "We now generate hindcasts for all combinations of forecast initialization dates and target periods."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Ensemble forecasting\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i in init_dates:\n",
    "\n",
    "    for p in target_periods:\n",
    "\n",
    "        # Define initialization date for which to produce hindcasts\n",
    "        init_day, init_month = int(i[0:2]), int(i[3:5])\n",
    "\n",
    "        # Define target period for which to produce hindcasts\n",
    "        target_start_day, target_start_month = int(p[0:2]), int(p[3:5])\n",
    "        target_end_day, target_end_month = int(p[6:8]), int(p[9:11])\n",
    "        target_start_month_name = datetime.datetime.strptime(str(target_start_month), \"%m\").strftime(\"%b\")\n",
    "        target_end_month_name = datetime.datetime.strptime(str(target_end_month), \"%m\").strftime(\"%b\")\n",
    "\n",
    "        # Check that the target period starts after the initialization date so we can proceed with the hindcasting\n",
    "        # Note: We assume that the initialization date and target period are both in the same year. No cross-year forecasting\n",
    "        if (target_start_month > init_month) or ((target_start_month == init_month) & (target_start_day >= init_day)):\n",
    "\n",
    "            counter += 1\n",
    "            \n",
    "            # Select predictor of interest\n",
    "            predictor_subset_df = predictor_df[(predictor_df.index.month == init_month) & (predictor_df.index.day == init_day)]\n",
    "\n",
    "            # Select predictand of interest\n",
    "            predictand_subset_df = predictand_df['Vol_'+str(target_start_day)+target_start_month_name+'-'+str(target_end_day)+target_end_month_name]\n",
    "\n",
    "            # Generate ensemble hindcasts\n",
    "            ens_hindcasts_df = ensemble_forecasting(predictor_subset_df, predictand_subset_df, PC_ids=PC_id_default, ens_size=ens_size_default, min_overlap_years=min_years_overlap_default, method_traintest=method_traintest_default, nyears_leaveout=nyears_leaveout_default)\n",
    "\n",
    "            # Save ensemble hindcasts to xarray DataArray\n",
    "            if ens_hindcasts_df is not None:\n",
    "                switch = 1\n",
    "                ens_hindcasts_da = xr.DataArray(data=ens_hindcasts_df.to_numpy().reshape((ens_hindcasts_df.to_numpy().shape[0], ens_hindcasts_df.to_numpy().shape[1], 1)), coords={'year':ens_hindcasts_df.index,'ens_member':ens_hindcasts_df.columns,'init_date':[i]}, dims=['year','ens_member','init_date'], name='Vol_'+str(target_start_day)+target_start_month_name+'-'+str(target_end_day)+target_end_month_name)\n",
    "            else:\n",
    "                ens_hindcasts_da = xr.DataArray(data=np.reshape([np.nan]*ens_size_default,(1,ens_size_default,1)), coords={'year':[2000],'ens_member':np.arange(1,ens_size_default+1),'init_date':[i]}, dims=['year','ens_member','init_date'], name='Vol_'+str(target_start_day)+target_start_month_name+'-'+str(target_end_day)+target_end_month_name)\n",
    "            ens_hindcasts_da.attrs['long_name'] = 'Ensemble volume hindcasts'\n",
    "            ens_hindcasts_da.attrs['info'] = 'Ensemble hindcasts of '+str(target_start_day)+target_start_month_name+'-'+str(target_end_day)+target_end_month_name+' volumes in basin '+test_basin_id+'. The hindcasts are generated using an Ordinary Least Squares (OLS) regression model, intialized with principal components ('+PC_id_default+') of gap filled SWE station observations on init_date as predictors.'\n",
    "            ens_hindcasts_da.attrs['units'] = 'm3'\n",
    "\n",
    "            # Save ensemble hindcasts to xarray Dataset\n",
    "            if counter == 1:\n",
    "                ens_hindcasts_ds = ens_hindcasts_da\n",
    "            else:\n",
    "                ens_hindcasts_ds = xr.merge([ens_hindcasts_ds, ens_hindcasts_da])\n",
    "\n",
    "    # Add information to the Dataset\n",
    "    ens_hindcasts_ds.init_date.attrs['long_name'] = 'Hindcast initialization date'\n",
    "    ens_hindcasts_ds.init_date.attrs['info'] = 'DD/MM of the predictors used to generate the hindcasts.'\n",
    "    ens_hindcasts_ds.ens_member.attrs['long_name'] = 'Ensemble member'\n",
    "    \n",
    "display(ens_hindcasts_ds)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data\n",
    "Save the output hindcasts so we can read them in other Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save the data\n",
    "ens_hindcasts_ds.to_netcdf(settings['output_data_path']+'ensemble_hindcasts_basin'+test_basin_id+'.nc', format=\"NETCDF4\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optionally store each model used to generate these hindcasts using the following code: model_fit.save('OLS_model.pkl', remove_data=False)\n",
    "To load model back we would do: loaded = sm.load('OLS_model.pkl')\n",
    "Note that a unique model is built for each initialization date - target period combination, but also for each year left out."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
